{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(path):\n",
    "    with open(path+\"/graphsum_multinews_sentences.log\", \"r\", encoding=\"utf-8\") as f:\n",
    "        rouge_cont = f.readlines()\n",
    "    rouge_cont = [x.strip() for x in rouge_cont] \n",
    "    indx=[i for i, s in enumerate(rouge_cont) if 'ROUGE' in s][-2:]\n",
    "    \n",
    "    with open(path+\"/job.log.0\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content] \n",
    "\n",
    "    indices = [i for i, s in enumerate(content) if 'word_embedding_name' in s]\n",
    "    indices_2 = [i for i, s in enumerate(content) if 'Num train examples' in s]\n",
    "    params=content[:indices[0]+1]+ [i[31:] for i in content[indices_2[0]:indices_2[0]+3]]+[rouge_cont[indx[0]][3:],rouge_cont[indx[1]]]\n",
    "    params= dict([p.split(\":\") for p in params])\n",
    "    params['epochs'] = np.ceil(int(params['Max train steps'])/1490).astype(int)\n",
    "    if path[11:]==\"\":\n",
    "        params['name']=\"Current run\"\n",
    "    else:\n",
    "        params['name']=path[11:]\n",
    "    indices = [i for i, s in enumerate(content) if 'loss:' in s]\n",
    "    loss_per_step =np.array([float(content[i][content[i].find(\"loss:\")+6:content[i].find(\", ppl\")]) for i in indices])\n",
    "    ppl_per_step=np.array([float(content[i][content[i].find(\"ppl:\")+5:content[i].find(\", acc\")]) for i in indices])\n",
    "    acc_per_step=np.array([float(content[i][content[i].find(\"acc:\")+5:content[i].find(\", lea\")]) for i in indices])\n",
    "    lr_per_step=np.array([float(content[i][content[i].find(\"rate:\")+6:content[i].find(\", spe\")]) for i in indices])\n",
    "    epochs=np.array([int(content[i][content[i].find(\"epoch:\")+7:content[i].find(\", pro\")]) for i in indices])\n",
    "    epochs_indexs=[np.where(epochs==i) for i in np.unique(epochs)]\n",
    "    acc_mean_per_epoch=[np.mean(acc_per_step[ep]) for ep in epochs_indexs]\n",
    "    loss_mean_per_epoch=[np.mean(loss_per_step[ep]) for ep in epochs_indexs]\n",
    "    ppl_mean_per_epoch=[np.mean(ppl_per_step[ep]) for ep in epochs_indexs]\n",
    "    lr_mean_per_epoch=[np.mean(lr_per_step[ep]) for ep in epochs_indexs]\n",
    "    values=[i for i in params.values()]\n",
    "    keys=[i for i in params.keys()]\n",
    "\n",
    "    return values, keys ,content,epochs_indexs, loss_per_step, ppl_per_step,lr_per_step, acc_mean_per_epoch, loss_mean_per_epoch, ppl_mean_per_epoch, lr_mean_per_epoch \n",
    "\n",
    "def plot_curves(logarithmic=True, start_step=0):\n",
    "    title=['loss_per_step', 'ppl_per_step','lr_per_step', 'acc_mean_per_epoch', 'loss_mean_per_epoch', 'ppl_mean_per_epoch', 'lr_mean_per_epoch']\n",
    "    for i in range(4,11):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.title(title[i-4])\n",
    "        for r in res:\n",
    "            plt.plot(r[i][start_step:], label=r[0][-1])\n",
    "        plt.legend()\n",
    "        if logarithmic:   \n",
    "            plt.yscale('log')\n",
    "            plt.ylabel(\"Logarithmic value\")\n",
    "        else:\n",
    "            plt.ylabel(\"Value\")\n",
    "        if i < 7:\n",
    "            plt.xlabel(\"Step\")\n",
    "        else:\n",
    "            plt.xlabel(\"Epoch\")\n",
    "        plt.grid()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=['./logs/log/job.log.0', './logs/log/18-Feb-10-epoch-sentence-pruned-75-30/job.log.0', './logs/log/22-Feb-100-epoch-sentence-pruned-75-30/job.log.0']\n",
    "paths=['./logs/log', './logs/log/18-Feb-10-epoch-sentence-pruned-75-30', './logs/log/22-Feb-100-epoch-sentence-pruned-75-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[parse_file(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame([i[0] for i in res], columns=res[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_probs_dropout_prob</th>\n",
       "      <th>dec_graph_layers</th>\n",
       "      <th>dec_word_pos_embedding_name</th>\n",
       "      <th>enc_graph_layers</th>\n",
       "      <th>enc_sen_pos_embedding_name</th>\n",
       "      <th>enc_word_layers</th>\n",
       "      <th>enc_word_pos_embedding_name</th>\n",
       "      <th>hidden_act</th>\n",
       "      <th>hidden_dropout_prob</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>initializer_range</th>\n",
       "      <th>max_position_embeddings</th>\n",
       "      <th>num_attention_heads</th>\n",
       "      <th>postprocess_command</th>\n",
       "      <th>preprocess_command</th>\n",
       "      <th>word_embedding_name</th>\n",
       "      <th>Num train examples</th>\n",
       "      <th>Max train steps</th>\n",
       "      <th>Num warmup steps</th>\n",
       "      <th>ROUGE-F(1/2/3/l)</th>\n",
       "      <th>ROUGE-R(1/2/3/l)</th>\n",
       "      <th>epochs</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>dec_word_pos_embedding</td>\n",
       "      <td>2</td>\n",
       "      <td>enc_sen_pos_embedding</td>\n",
       "      <td>6</td>\n",
       "      <td>enc_word_pos_embedding</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>da</td>\n",
       "      <td>n</td>\n",
       "      <td>word_embedding</td>\n",
       "      <td>44692</td>\n",
       "      <td>446920</td>\n",
       "      <td>8000</td>\n",
       "      <td>40.76/13.98/37.23</td>\n",
       "      <td>37.76/12.99/34.45</td>\n",
       "      <td>300</td>\n",
       "      <td>Current run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>dec_word_pos_embedding</td>\n",
       "      <td>2</td>\n",
       "      <td>enc_sen_pos_embedding</td>\n",
       "      <td>6</td>\n",
       "      <td>enc_word_pos_embedding</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>da</td>\n",
       "      <td>n</td>\n",
       "      <td>word_embedding</td>\n",
       "      <td>44692</td>\n",
       "      <td>14897</td>\n",
       "      <td>8000</td>\n",
       "      <td>42.95/15.32/39.42</td>\n",
       "      <td>38.17/13.63/34.98</td>\n",
       "      <td>10</td>\n",
       "      <td>18-Feb-10-epoch-sentence-pruned-75-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>dec_word_pos_embedding</td>\n",
       "      <td>2</td>\n",
       "      <td>enc_sen_pos_embedding</td>\n",
       "      <td>6</td>\n",
       "      <td>enc_word_pos_embedding</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>da</td>\n",
       "      <td>n</td>\n",
       "      <td>word_embedding</td>\n",
       "      <td>44692</td>\n",
       "      <td>148973</td>\n",
       "      <td>8000</td>\n",
       "      <td>38.62/13.17/35.20</td>\n",
       "      <td>35.67/12.20/32.46</td>\n",
       "      <td>100</td>\n",
       "      <td>22-Feb-100-epoch-sentence-pruned-75-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  attention_probs_dropout_prob dec_graph_layers dec_word_pos_embedding_name  \\\n",
       "0                          0.1                8      dec_word_pos_embedding   \n",
       "1                          0.1                8      dec_word_pos_embedding   \n",
       "2                          0.1                8      dec_word_pos_embedding   \n",
       "\n",
       "  enc_graph_layers enc_sen_pos_embedding_name enc_word_layers  \\\n",
       "0                2      enc_sen_pos_embedding               6   \n",
       "1                2      enc_sen_pos_embedding               6   \n",
       "2                2      enc_sen_pos_embedding               6   \n",
       "\n",
       "  enc_word_pos_embedding_name hidden_act hidden_dropout_prob hidden_size  \\\n",
       "0      enc_word_pos_embedding       relu                 0.1         256   \n",
       "1      enc_word_pos_embedding       relu                 0.1         256   \n",
       "2      enc_word_pos_embedding       relu                 0.1         256   \n",
       "\n",
       "  initializer_range max_position_embeddings num_attention_heads  \\\n",
       "0              0.02                     512                   8   \n",
       "1              0.02                     512                   8   \n",
       "2              0.02                     512                   8   \n",
       "\n",
       "  postprocess_command preprocess_command word_embedding_name  \\\n",
       "0                  da                  n      word_embedding   \n",
       "1                  da                  n      word_embedding   \n",
       "2                  da                  n      word_embedding   \n",
       "\n",
       "  Num train examples Max train steps Num warmup steps    ROUGE-F(1/2/3/l)  \\\n",
       "0              44692          446920             8000   40.76/13.98/37.23   \n",
       "1              44692           14897             8000   42.95/15.32/39.42   \n",
       "2              44692          148973             8000   38.62/13.17/35.20   \n",
       "\n",
       "     ROUGE-R(1/2/3/l)  epochs                                    name  \n",
       "0   37.76/12.99/34.45     300                             Current run  \n",
       "1   38.17/13.63/34.98      10   18-Feb-10-epoch-sentence-pruned-75-30  \n",
       "2   35.67/12.20/32.46     100  22-Feb-100-epoch-sentence-pruned-75-30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric curves in logarithmic scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d9ed95f1be48b7947501604f20971e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c270945c40c544feaa2a444661ecfefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66dd2e73bc94e0f890d388aa3f99deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2f812b6eed4d95bab31b363f266ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10bd10343694fa894863c05211f939a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968b7f200db54dc38eb1e61caf50c795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lautarohickmann/anaconda3/envs/AMDML/lib/python3.7/site-packages/ipykernel_launcher.py:40: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b0caffced6419b8fe775f544c0d9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curves(logarithmic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fd8eacc48d46aa9b0baf0ead67ef5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3972ac823c0d4e2a9afe7230c8a12c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3420a18921406784e722d2ce0750c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdb5499e9b04275a29c18800eba38ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a361b8e08b474132973b73ac1d56ee8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65684d601a14c518c2f70b54d0f7642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cc1b563b6a472e973372ccc1d9d1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curves(logarithmic=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
