{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(path):\n",
    "    with open(path+\"/graphsum_multinews_sentences.log\", \"r\", encoding=\"utf-8\") as f:\n",
    "        rouge_cont = f.readlines()\n",
    "    rouge_cont = [x.strip() for x in rouge_cont] \n",
    "    indx=[i for i, s in enumerate(rouge_cont) if 'ROUGE' in s][-2:]\n",
    "    \n",
    "    with open(path+\"/job.log.0\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content] \n",
    "\n",
    "    indices = [i for i, s in enumerate(content) if 'word_embedding_name' in s]\n",
    "    indices_2 = [i for i, s in enumerate(content) if 'Num train examples' in s]\n",
    "    params=content[:indices[0]+1]+ [i[31:] for i in content[indices_2[0]:indices_2[0]+3]]+[rouge_cont[indx[0]][3:],rouge_cont[indx[1]]]\n",
    "    params= dict([p.split(\":\") for p in params])\n",
    "    params['epochs'] = np.ceil(int(params['Max train steps'])/1490).astype(int)\n",
    "    params['name']=path[11:]\n",
    "    indices = [i for i, s in enumerate(content) if 'loss:' in s]\n",
    "    loss_per_step =np.array([float(content[i][content[i].find(\"loss:\")+6:content[i].find(\", ppl\")]) for i in indices])\n",
    "    ppl_per_step=np.array([float(content[i][content[i].find(\"ppl:\")+5:content[i].find(\", acc\")]) for i in indices])\n",
    "    acc_per_step=np.array([float(content[i][content[i].find(\"acc:\")+5:content[i].find(\", lea\")]) for i in indices])\n",
    "    lr_per_step=np.array([float(content[i][content[i].find(\"rate:\")+6:content[i].find(\", spe\")]) for i in indices])\n",
    "    epochs=np.array([int(content[i][content[i].find(\"epoch:\")+7:content[i].find(\", pro\")]) for i in indices])\n",
    "    epochs_indexs=[np.where(epochs==i) for i in np.unique(epochs)]\n",
    "    acc_mean_per_epoch=[np.mean(acc_per_step[ep]) for ep in epochs_indexs]\n",
    "    loss_mean_per_epoch=[np.mean(loss_per_step[ep]) for ep in epochs_indexs]\n",
    "    ppl_mean_per_epoch=[np.mean(ppl_per_step[ep]) for ep in epochs_indexs]\n",
    "    lr_mean_per_epoch=[np.mean(lr_per_step[ep]) for ep in epochs_indexs]\n",
    "    values=[i for i in params.values()]\n",
    "    keys=[i for i in params.keys()]\n",
    "\n",
    "    return values, keys ,content, loss_per_step, ppl_per_step,lr_per_step, epochs_indexs, acc_mean_per_epoch, loss_mean_per_epoch, ppl_mean_per_epoch, lr_mean_per_epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=['./logs/log/job.log.0', './logs/log/18-Feb-10-epoch-sentence-pruned-75-30/job.log.0', './logs/log/22-Feb-100-epoch-sentence-pruned-75-30/job.log.0']\n",
    "paths=['./logs/log', './logs/log/18-Feb-10-epoch-sentence-pruned-75-30', './logs/log/22-Feb-100-epoch-sentence-pruned-75-30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[parse_file(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame([i[0] for i in res], columns=res[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_probs_dropout_prob</th>\n",
       "      <th>dec_graph_layers</th>\n",
       "      <th>dec_word_pos_embedding_name</th>\n",
       "      <th>enc_graph_layers</th>\n",
       "      <th>enc_sen_pos_embedding_name</th>\n",
       "      <th>enc_word_layers</th>\n",
       "      <th>enc_word_pos_embedding_name</th>\n",
       "      <th>hidden_act</th>\n",
       "      <th>hidden_dropout_prob</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>initializer_range</th>\n",
       "      <th>max_position_embeddings</th>\n",
       "      <th>num_attention_heads</th>\n",
       "      <th>postprocess_command</th>\n",
       "      <th>preprocess_command</th>\n",
       "      <th>word_embedding_name</th>\n",
       "      <th>Num train examples</th>\n",
       "      <th>Max train steps</th>\n",
       "      <th>Num warmup steps</th>\n",
       "      <th>ROUGE-F(1/2/3/l)</th>\n",
       "      <th>ROUGE-R(1/2/3/l)</th>\n",
       "      <th>epochs</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>dec_word_pos_embedding</td>\n",
       "      <td>2</td>\n",
       "      <td>enc_sen_pos_embedding</td>\n",
       "      <td>6</td>\n",
       "      <td>enc_word_pos_embedding</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>da</td>\n",
       "      <td>n</td>\n",
       "      <td>word_embedding</td>\n",
       "      <td>44692</td>\n",
       "      <td>446920</td>\n",
       "      <td>8000</td>\n",
       "      <td>40.76/13.98/37.23</td>\n",
       "      <td>37.76/12.99/34.45</td>\n",
       "      <td>300</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>dec_word_pos_embedding</td>\n",
       "      <td>2</td>\n",
       "      <td>enc_sen_pos_embedding</td>\n",
       "      <td>6</td>\n",
       "      <td>enc_word_pos_embedding</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>da</td>\n",
       "      <td>n</td>\n",
       "      <td>word_embedding</td>\n",
       "      <td>44692</td>\n",
       "      <td>14897</td>\n",
       "      <td>8000</td>\n",
       "      <td>42.95/15.32/39.42</td>\n",
       "      <td>38.17/13.63/34.98</td>\n",
       "      <td>10</td>\n",
       "      <td>18-Feb-10-epoch-sentence-pruned-75-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>8</td>\n",
       "      <td>dec_word_pos_embedding</td>\n",
       "      <td>2</td>\n",
       "      <td>enc_sen_pos_embedding</td>\n",
       "      <td>6</td>\n",
       "      <td>enc_word_pos_embedding</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>da</td>\n",
       "      <td>n</td>\n",
       "      <td>word_embedding</td>\n",
       "      <td>44692</td>\n",
       "      <td>148973</td>\n",
       "      <td>8000</td>\n",
       "      <td>38.62/13.17/35.20</td>\n",
       "      <td>35.67/12.20/32.46</td>\n",
       "      <td>100</td>\n",
       "      <td>22-Feb-100-epoch-sentence-pruned-75-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  attention_probs_dropout_prob dec_graph_layers dec_word_pos_embedding_name  \\\n",
       "0                          0.1                8      dec_word_pos_embedding   \n",
       "1                          0.1                8      dec_word_pos_embedding   \n",
       "2                          0.1                8      dec_word_pos_embedding   \n",
       "\n",
       "  enc_graph_layers enc_sen_pos_embedding_name enc_word_layers  \\\n",
       "0                2      enc_sen_pos_embedding               6   \n",
       "1                2      enc_sen_pos_embedding               6   \n",
       "2                2      enc_sen_pos_embedding               6   \n",
       "\n",
       "  enc_word_pos_embedding_name hidden_act hidden_dropout_prob hidden_size  \\\n",
       "0      enc_word_pos_embedding       relu                 0.1         256   \n",
       "1      enc_word_pos_embedding       relu                 0.1         256   \n",
       "2      enc_word_pos_embedding       relu                 0.1         256   \n",
       "\n",
       "  initializer_range max_position_embeddings num_attention_heads  \\\n",
       "0              0.02                     512                   8   \n",
       "1              0.02                     512                   8   \n",
       "2              0.02                     512                   8   \n",
       "\n",
       "  postprocess_command preprocess_command word_embedding_name  \\\n",
       "0                  da                  n      word_embedding   \n",
       "1                  da                  n      word_embedding   \n",
       "2                  da                  n      word_embedding   \n",
       "\n",
       "  Num train examples Max train steps Num warmup steps    ROUGE-F(1/2/3/l)  \\\n",
       "0              44692          446920             8000   40.76/13.98/37.23   \n",
       "1              44692           14897             8000   42.95/15.32/39.42   \n",
       "2              44692          148973             8000   38.62/13.17/35.20   \n",
       "\n",
       "     ROUGE-R(1/2/3/l)  epochs                                    name  \n",
       "0   37.76/12.99/34.45     300                                          \n",
       "1   38.17/13.63/34.98      10   18-Feb-10-epoch-sentence-pruned-75-30  \n",
       "2   35.67/12.20/32.46     100  22-Feb-100-epoch-sentence-pruned-75-30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
