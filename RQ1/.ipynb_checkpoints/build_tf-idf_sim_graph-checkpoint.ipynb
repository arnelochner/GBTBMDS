{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from os.path import join as pjoin\n",
    "import codecs\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing.pool\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import groupby\n",
    "\n",
    "import sentencepiece\n",
    "from gensim import corpora, models, similarities\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_para = json.load(open(r\"E:\\graphsum\\data\\analyze_sim_graph_weights\\MultiNews.30.train.11.json\", encoding=\"utf-8\"))\n",
    "json_sent = json.load(open(r\"E:\\graphsum\\data\\analyze_sim_graph_weights\\sent_MultiNews.30.train.11.json\", encoding=\"utf-8\"))\n",
    "\n",
    "json_paddle = json.load(open(r\"E:\\graphsum\\data\\MultiNews_data_tfidf_30_paddle\\train\\MultiNews.30.train.11.json\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8206063088897992"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(ex[\"sim_graph\"]) for ex in json_sent]) / np.sum([len(ex[\"sim_graph\"]) for ex in json_para])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sentencepiece.SentencePieceProcessor()\n",
    "spm.load(r\"E:\\graphsum\\data\\spm9998_3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_input(corpus):\n",
    "    raw_corpus = [' '.join(para) for para in corpus]\n",
    "       \n",
    "    # create English stop words list\n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    \n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # Lowercase each document, split it by white space and filter out stopwords\n",
    "    texts = [[word for word in para.lower().split() if word not in stoplist]\n",
    "             for para in raw_corpus]\n",
    "    # Create a set of frequent words\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    # stem each word\n",
    "    processed_corpus = [[p_stemmer.stem(\n",
    "        token) for token in text if frequency[token] > 1] for text in texts]\n",
    "   \n",
    "    dictionary = corpora.Dictionary(processed_corpus)\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "    # train the model\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    # transform the \"system minors\" string\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    index = similarities.SparseMatrixSimilarity(\n",
    "        corpus_tfidf, num_features=len(dictionary))\n",
    "    \n",
    "    to_remove = []\n",
    "    for i, cor in enumerate(corpus_tfidf):\n",
    "        sim = index[cor]\n",
    "        sim_to_other_values = np.mean(sim)\n",
    "        if (len(processed_corpus[i]) < 3) or (sim_to_other_values < 0.05) or (sim_to_other_values > 0.15):\n",
    "            #print(\"The corpus_tfidf[i] is None: %d\" % i)\n",
    "            #print(bow_corpus[i])\n",
    "            to_remove.append(i)\n",
    "            # exit(1)\n",
    "\n",
    "    \n",
    "    valid_indexes = [x for x in range(len(corpus_tfidf)) if x not in to_remove]\n",
    "    \n",
    "    return [sent for i,sent in enumerate(corpus) if i in valid_indexes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tfidf_sim_graph_by_gensim(corpus, sim_threshold):\n",
    "    \"\"\"Constuct TFIDF similarity graph by Gensim package\"\"\"\n",
    "\n",
    "    sim_graph = []\n",
    "    \n",
    "    ### Trenne Buchstabend von einander\n",
    "    raw_corpus = [' '.join(para) for para in corpus]\n",
    "    \n",
    "    # create English stop words list\n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    \n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # Lowercase each document, split it by white space and filter out stopwords\n",
    "    texts = [[word for word in para.lower().split() if word not in stoplist]\n",
    "             for para in raw_corpus]\n",
    "    # Create a set of frequent words\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    # stem each word\n",
    "    processed_corpus = [[p_stemmer.stem(\n",
    "        token) for token in text if frequency[token] > 1] for text in texts]\n",
    "   \n",
    "    dictionary = corpora.Dictionary(processed_corpus)\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "    # train the model\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    # transform the \"system minors\" string\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    \n",
    "    index = similarities.SparseMatrixSimilarity(\n",
    "        corpus_tfidf, num_features=len(dictionary))\n",
    "\n",
    "    total = 0.\n",
    "    count_large = 0.\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        sim = index[corpus_tfidf[i]]\n",
    "        assert len(sim) == len(corpus_tfidf), \"the tfidf sim is not correct!\"\n",
    "        sim_graph.append(sim.tolist())\n",
    "\n",
    "        for s in sim:\n",
    "            total += 1\n",
    "            if s > sim_threshold:\n",
    "                count_large += 1\n",
    "\n",
    "    print(\"sim_graph[0]: %s\" % str(sim_graph[0]))\n",
    "\n",
    "    return sim_graph, count_large, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_filtered_short = [[spm.decode_ids(\n",
    "                [token]) for token in sent] for sent in json_sent[3][\"src\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_input = prune_input(src_filtered_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_graph, count_large, total = construct_tfidf_sim_graph_by_gensim(pruned_input,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(sim_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_src_tokens = [[spm.decode_ids(\n",
    "                [token]) for token in sent] for sent in json_para[3][\"src\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = 0\n",
    "total_sent = 0\n",
    "for ex in json_sent:\n",
    "    src_filtered_short = src_filtered_short = [[spm.decode_ids(\n",
    "                [token]) for token in sent] for sent in ex[\"src\"]]\n",
    "    unpruned_length = len(src_filtered_short)\n",
    "    \n",
    "    pruned = prune_input(src_filtered_short)\n",
    "    \n",
    "    pruned_length = unpruned_length - len(pruned)\n",
    "    \n",
    "    total_sent += unpruned_length\n",
    "    \n",
    "    save += pruned_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_sent: 44441, save: 9363, percentage: 0.21068382799666974\n"
     ]
    }
   ],
   "source": [
    "print(f\"total_sent: {total_sent}, save: {save}, percentage: {save/total_sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sim_graph, count_large, total = construct_tfidf_sim_graph_by_gensim(para_src_tokens,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(paddle_sim_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bitb758941065764c65be30b5af71a2fbcf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
